{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNv2dUZAGpdBlAwysQI/EsO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adithyanmk/javascript-code-assist-ai/blob/main/Untitled19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "# Function to get the last stopped epoch from checkpoint filenames\n",
        "def get_last_epoch_from_checkpoints(checkpoint_dir):\n",
        "    checkpoint_files = os.listdir(checkpoint_dir)\n",
        "    if checkpoint_files:\n",
        "        epoch_numbers = [int(filename.split('_')[2].split('.')[0]) for filename in checkpoint_files if 'model_checkpoint' in filename]\n",
        "        last_epoch = max(epoch_numbers)\n",
        "        return last_epoch\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# Function to load the latest model checkpoints\n",
        "def load_latest_generator_checkpoint(model, checkpoint_dir):\n",
        "    last_epoch = get_last_epoch_from_checkpoints(checkpoint_dir)\n",
        "    if last_epoch > 0:\n",
        "        model.load_weights(os.path.join(checkpoint_dir, f\"model_checkpoint_{last_epoch:03d}.h5\"))\n",
        "        print(f\"Resuming training from epoch {last_epoch}.\")\n",
        "    return model, last_epoch\n",
        "\n",
        "# Read text data from a file\n",
        "with open('javascript_dataset.txt', 'r') as file:\n",
        "    texts = file.readlines()\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create input sequences and labels\n",
        "input_sequences = []\n",
        "for text in texts:\n",
        "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_length = max([len(x) for x in input_sequences])\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n",
        "\n",
        "# Build a simple LSTM model\n",
        "model = keras.Sequential([\n",
        "    Embedding(total_words, 100, input_length=max_sequence_length-1),\n",
        "    LSTM(100),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Define a folder for model checkpoints\n",
        "checkpoint_folder = 'checkpoints'\n",
        "os.makedirs(checkpoint_folder, exist_ok=True)\n",
        "\n",
        "# Define a CustomModelCheckpoint callback to save checkpoints with epoch names\n",
        "class CustomModelCheckpoint(Callback):\n",
        "    def __init__(self, filepath, save_freq):\n",
        "        super(CustomModelCheckpoint, self).__init__()\n",
        "        self.filepath = filepath\n",
        "        self.save_freq = save_freq\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) % self.save_freq == 0:\n",
        "            filepath = self.filepath.format(epoch=epoch + 1, **logs)\n",
        "            self.model.save(filepath)\n",
        "\n",
        "# Load the latest model checkpoint\n",
        "model, last_epoch = load_latest_generator_checkpoint(model, checkpoint_folder)\n",
        "\n",
        "# Define a CustomModelCheckpoint callback to save checkpoints with epoch names\n",
        "checkpoint_filepath = os.path.join(checkpoint_folder, \"model_checkpoint_{epoch:03d}.h5\")\n",
        "custom_checkpoint_callback = CustomModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_freq=100,  # Save every 100 epochs\n",
        ")\n",
        "\n",
        "# Train the model with the custom checkpoint callback, specifying the initial epoch\n",
        "model.fit(X, y, epochs=1000, verbose=1, callbacks=[custom_checkpoint_callback], initial_epoch=last_epoch)\n",
        "\n",
        "# Generate text from a seed text\n",
        "seed_text = \"Hello,\"\n",
        "next_words = 5\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
        "    predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "    predicted_index = np.argmax(predicted_probs)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted_index:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ],
      "metadata": {
        "id": "3RbZNpWsRolq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Read text data from a file\n",
        "with open('javascript_dataset.txt', 'r') as file:\n",
        "    texts = file.readlines()\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create input sequences and labels (same code as before)\n",
        "input_sequences = []\n",
        "for text in texts:\n",
        "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "max_sequence_length = max([len(x) for x in input_sequences])\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n",
        "\n",
        "# Build a simple LSTM model (same code as before)\n",
        "model = keras.Sequential([\n",
        "    Embedding(total_words, 100, input_length=max_sequence_length-1),\n",
        "    LSTM(100),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Load the saved checkpoint\n",
        "checkpoint_filepath = 'checkpoints/model_checkpoint_300.h5'\n",
        "model.load_weights(checkpoint_filepath)\n",
        "\n",
        "# Generate text from a seed text\n",
        "seed_text = \"make a variable\"\n",
        "next_words = 10\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
        "    predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "    predicted_index = np.argmax(predicted_probs)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted_index:\n",
        "            output_word = word\n",
        "            break\n",
        "    seed_text += \" \" + output_word\n",
        "\n",
        "print(seed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6BfAtMlZ823",
        "outputId": "2df01ca5-a42b-4db3-c9bc-fec0ef50f60e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "make a variable declared without a value will have the value undefined after\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote add origin https://github.com/adithyanmk/javascript-code-assist-ai.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90xq9SE5mNdJ",
        "outputId": "c742a59a-26ee-499f-d104-f4cdb7216a52"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    }
  ]
}